

- [11.多线程、多进程、协程](#11多线程多进程协程)
  - [1 多线程（multi threading）](#1-多线程multi-threading)
    - [（1）threading](#1threading)
    - [（2）线程锁（Lock）](#2线程锁lock)
  - [2 多进程（multi processing）](#2-多进程multi-processing)
    - [（1）Queue](#1queue)
    - [（2）进程池](#2进程池)
  - [3 定时任务 + 多线程](#3-定时任务--多线程)



# 11.多线程、多进程、协程

Global Interpreter Lock(全局解释器锁)
## 1 多线程（multi threading）

### （1）threading

```python
import threading

#t = threading.Thread(target=Timing_to_process, args=(111,112))  # 创建线程
Thread1 = threading.Thread(target = sent_Datas_to_server2)  # 创建线程
Thread1.setDaemon(True)                                     # 设置为后台线程，这里默认是False，设置为True之后则主线程不用等待子线程
Thread1.start()                                             # 开启线程

Thread1.join()    # Join的作用是阻塞进程直到线程执行完毕。

```


### （2）线程锁（Lock）

多个线程总同时调用一个global
```python
global lock
lock = threading.Lock() #创建一个锁

def thread1_func():
    global cnt2
    global lock
    with lock:  #with Lock的作用相当于自动获取和释放锁(资源)
        cnt2 += 1

def thread1_func():
    global cnt2
    global lock
    with lock:  #with Lock的作用相当于自动获取和释放锁(资源)
        cnt2 += 1
```


## 2 多进程（multi processing）

https://blog.csdn.net/weixin_38611497/article/details/81490960
### （1）Queue

```python
import multiprocessing as mp

def job(q):
    res = 0
    for i in range(1000000):
        res += i + i**2 + i**3
    q.put(res) # queue 的功能是将每个核或线程的运算结果放在队里中， 等到每个线程或核运行完毕后再从队列中取出结果， 继续加载运算。

def multicore():
    q = mp.Queue()
    p1 = mp.Process(target=job, args=(q,))
    p2 = mp.Process(target=job, args=(q,))
    p1.start()
    p2.start()
    p1.join()
    p2.join()
    res1 = q.get()
    res2 = q.get()
    print('multicore:',res1 + res2)
```


### （2）进程池

```python
def multicore():
    pool = mp.Pool() 
    res = pool.map(job, range(10))
    print(res)
    res = pool.apply_async(job, (2,))
    # 用get获得结果
    print(res.get())
    # 迭代器，i=0时apply一次，i=1时apply一次等等
    multi_res = [pool.apply_async(job, (i,)) for i in range(10)]
    # 从迭代器中取出
    print([res.get() for res in multi_res])


def dataset_global_features(dataset, feature_files, cores):
    """
      Args:
        dataset: dataset object that contains the video ids
        feature_files: a dictionary that contains the feature path of the given dataset videos
        cores: CPU cores for the parallel extraction
      Returns:
        global_features: global features of the videos in the given dataset
    """
    progress_bar = tqdm(xrange(len(dataset['index'])), unit='video')
    # extract features in parallel
    pool = Pool(cores)
    future = []
    for video_id in dataset['index']:
        print(len(dataset['index']))
        print('------------video_id--------')
        print(video_id)
        
        future += [pool.apply_async(global_vector,
                        args=[feature_files.get(video_id)],
                        callback=(lambda *a: progress_bar.update()))]
    pool.close()
    pool.join()

    # find feature dimension
    dim = 0
    for f in future:
        print("=====f.get().shape====",f.get().shape[0])
        print(f.get().shape[1])
        print(f.get())
        
        if f.get().size > 0:
            dim = f.get().shape[1]
            break

    # collect global features
    global_features = np.zeros((len(future), dim))
    for i, f in enumerate(future):
        if f.get().size > 0:
            global_features[i] = f.get()
            print("===f.get().shape===",type(f.get()))
            
    progress_bar.close()
    pool.terminate()
```


**注意：**
多进程 multiprocessing 并不能加速网络训练的preprocessing：
https://www.cnblogs.com/Yanjy-OnlyOne/p/13493971.html
原因：前处理虽然有读图像的的操作，主要还是计算，所以是CPU密集型，非IO密集。如果是爬虫类型任务的比较适合。



## 3 定时任务 + 多线程

（见脚本 utils_cv）


